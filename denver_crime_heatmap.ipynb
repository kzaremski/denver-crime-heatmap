{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denver Crime Heatmap Notebook\n",
    "CS 3120 Machine Learning Term Project - Konstantin Zaremski\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive notebook covers my project from ideation and planning, to exploratory data analysis (EDA), and final implementation.\n",
    "\n",
    "> ### Project Proposal\n",
    "> ##### Introduction & Description\n",
    "> After my car was vandalized in Denver earlier this October, I began ruminating on what makes an area “safe”. Eventually I came upon the Denver Public Crime Map which shows the 1000 most recent incidents on a map. This got me interested in the possibility of using this data in a predictive capacity. I am going to build a Denver crime prediction or likelihood map app, that will predict the likelihood of crime overall, predict the most likely crimes to be committed for different areas, and then present this as a heatmap visualization to the user.\n",
    "> \n",
    "> ##### Dataset\n",
    "> The source dataset for the public crime map is provided by Denver and contains 394,475 data points sourced from the FBI’s NIBRS database. After stripping off useless columns such as database keys and precinct numbers, the usable columns are the date and time of occurrence, type and category of crime, and longitude and latitude. Crimes do not happen because of a longitude and latitude value, so I am enriching the original dataset by adding additional features by querying each data point against Open Street Map data for land use, proximity to residential or main roads, building types, and amenities such as ATMs or RTD stations.\n",
    "> \n",
    "> ##### Model\n",
    "> I will be training two models to make predictions for my app: one model to enable the prediction of the total amount of crimes in an area given input features, and another model to perform multiclass classification to predict what type of crime is most likely to be committed in an area given input features. Since the input data and resulting crime or number of crimes is known and labeled, supervised learning models are best suited for this data. I will be using SciKit Learn’s HistGradientBoostingClassifier to predict crime type and HistGradientBoostingRegressor to predict crime volume. I am electing to use gradient boosting over decision trees or other models since it has a lower bias and should be more sensitive to different crime patterns.\n",
    "> \n",
    "> ##### App Functionality\n",
    "> The web app will be a simple map interface with a color heatmap overlay based on the underlying model’s crime predictions. The user will have options for a time delta, where they can view the heatmap/predictions for the current time plus or minus up to 48 hours. The user will have a way to toggle between seeing a multi-color heatmap for the types of crimes that are predicted, a single-color heatmap for the likelihood or predicted frequency of crime, and a heatmap that combines both, with areas having a lower predicted likelihood or frequency of crime being denoted by a more transparent version of the color representing the crime type for that area.\n",
    "> \n",
    "> ##### Technical Implementation\n",
    "> Since the app does not have user accounts, the backend can be implemented as a simple Flask app with an API endpoint to retrieve the heatmap overlay based on the map area the client says the user is currently viewing. Predictions will be stored in an SQLite database where they can be quickly retrieved to generate a heatmap at view time. The frontend will be implemented using simple HTML, CSS, and vanilla JavaScript to enable interactions. The map itself will be rendered using the Leaflet JavaScript map library/project.\n",
    "> \n",
    "> ##### Feasibility\n",
    "> Since the app itself is a visualization with only a few buttons/interactions, and no user accounts or user data collection, it will be simple to implement, which will allow for more time to be spent on tuning the models and finding the optimal way to slice the map or bin data.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis & Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Importing required modules and libraries for EDA, etc. See `requirements_eda.txt` for a full list of requirements to install into the environment using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import math\n",
    "import pyproj\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>INCIDENT_ID</th>\n",
       "      <th>OFFENSE_ID</th>\n",
       "      <th>OFFENSE_CODE</th>\n",
       "      <th>OFFENSE_CODE_EXTENSION</th>\n",
       "      <th>OFFENSE_TYPE_ID</th>\n",
       "      <th>OFFENSE_CATEGORY_ID</th>\n",
       "      <th>FIRST_OCCURRENCE_DATE</th>\n",
       "      <th>LAST_OCCURRENCE_DATE</th>\n",
       "      <th>REPORTED_DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>GEO_LON</th>\n",
       "      <th>GEO_LAT</th>\n",
       "      <th>DISTRICT_ID</th>\n",
       "      <th>PRECINCT_ID</th>\n",
       "      <th>NEIGHBORHOOD_ID</th>\n",
       "      <th>IS_CRIME</th>\n",
       "      <th>IS_TRAFFIC</th>\n",
       "      <th>VICTIM_COUNT</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2020467360</td>\n",
       "      <td>2020467360299901</td>\n",
       "      <td>2999</td>\n",
       "      <td>1</td>\n",
       "      <td>criminal-mischief-mtr-veh</td>\n",
       "      <td>public-disorder</td>\n",
       "      <td>8/2/2020 10:43:00 PM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/2/2020 10:43:00 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>-105.024597</td>\n",
       "      <td>39.689751</td>\n",
       "      <td>4</td>\n",
       "      <td>422</td>\n",
       "      <td>ruby-hill</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.133789e+06</td>\n",
       "      <td>1.676471e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>20196003434</td>\n",
       "      <td>20196003434299901</td>\n",
       "      <td>2999</td>\n",
       "      <td>1</td>\n",
       "      <td>criminal-mischief-mtr-veh</td>\n",
       "      <td>public-disorder</td>\n",
       "      <td>4/20/2019 7:30:00 AM</td>\n",
       "      <td>4/20/2019 8:45:00 AM</td>\n",
       "      <td>4/20/2019 6:30:00 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>-104.987348</td>\n",
       "      <td>39.714316</td>\n",
       "      <td>3</td>\n",
       "      <td>311</td>\n",
       "      <td>speer</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.144221e+06</td>\n",
       "      <td>1.685476e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002</td>\n",
       "      <td>2020123049</td>\n",
       "      <td>2020123049299901</td>\n",
       "      <td>2999</td>\n",
       "      <td>1</td>\n",
       "      <td>criminal-mischief-mtr-veh</td>\n",
       "      <td>public-disorder</td>\n",
       "      <td>2/25/2020 11:30:00 PM</td>\n",
       "      <td>2/26/2020 6:45:00 AM</td>\n",
       "      <td>2/26/2020 7:30:00 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>-104.988098</td>\n",
       "      <td>39.764528</td>\n",
       "      <td>6</td>\n",
       "      <td>612</td>\n",
       "      <td>five-points</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.143907e+06</td>\n",
       "      <td>1.703765e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003</td>\n",
       "      <td>2021621685</td>\n",
       "      <td>2021621685299901</td>\n",
       "      <td>2999</td>\n",
       "      <td>1</td>\n",
       "      <td>criminal-mischief-mtr-veh</td>\n",
       "      <td>public-disorder</td>\n",
       "      <td>11/1/2021 6:20:00 AM</td>\n",
       "      <td>11/1/2021 6:30:00 AM</td>\n",
       "      <td>11/1/2021 10:15:00 AM</td>\n",
       "      <td>...</td>\n",
       "      <td>-105.004665</td>\n",
       "      <td>39.739669</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>lincoln-park</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.139299e+06</td>\n",
       "      <td>1.694684e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004</td>\n",
       "      <td>2020289138</td>\n",
       "      <td>2020289138299901</td>\n",
       "      <td>2999</td>\n",
       "      <td>1</td>\n",
       "      <td>criminal-mischief-mtr-veh</td>\n",
       "      <td>public-disorder</td>\n",
       "      <td>5/9/2020 12:00:00 PM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/11/2020 12:05:00 PM</td>\n",
       "      <td>...</td>\n",
       "      <td>-104.830661</td>\n",
       "      <td>39.795281</td>\n",
       "      <td>5</td>\n",
       "      <td>521</td>\n",
       "      <td>montbello</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.188083e+06</td>\n",
       "      <td>1.715255e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID  INCIDENT_ID         OFFENSE_ID  OFFENSE_CODE  \\\n",
       "0     20000   2020467360   2020467360299901          2999   \n",
       "1     20001  20196003434  20196003434299901          2999   \n",
       "2     20002   2020123049   2020123049299901          2999   \n",
       "3     20003   2021621685   2021621685299901          2999   \n",
       "4     20004   2020289138   2020289138299901          2999   \n",
       "\n",
       "   OFFENSE_CODE_EXTENSION            OFFENSE_TYPE_ID OFFENSE_CATEGORY_ID  \\\n",
       "0                       1  criminal-mischief-mtr-veh     public-disorder   \n",
       "1                       1  criminal-mischief-mtr-veh     public-disorder   \n",
       "2                       1  criminal-mischief-mtr-veh     public-disorder   \n",
       "3                       1  criminal-mischief-mtr-veh     public-disorder   \n",
       "4                       1  criminal-mischief-mtr-veh     public-disorder   \n",
       "\n",
       "   FIRST_OCCURRENCE_DATE  LAST_OCCURRENCE_DATE          REPORTED_DATE  ...  \\\n",
       "0   8/2/2020 10:43:00 PM                   NaN   8/2/2020 10:43:00 PM  ...   \n",
       "1   4/20/2019 7:30:00 AM  4/20/2019 8:45:00 AM   4/20/2019 6:30:00 PM  ...   \n",
       "2  2/25/2020 11:30:00 PM  2/26/2020 6:45:00 AM   2/26/2020 7:30:00 AM  ...   \n",
       "3   11/1/2021 6:20:00 AM  11/1/2021 6:30:00 AM  11/1/2021 10:15:00 AM  ...   \n",
       "4   5/9/2020 12:00:00 PM                   NaN  5/11/2020 12:05:00 PM  ...   \n",
       "\n",
       "      GEO_LON    GEO_LAT  DISTRICT_ID  PRECINCT_ID  NEIGHBORHOOD_ID IS_CRIME  \\\n",
       "0 -105.024597  39.689751            4          422        ruby-hill        1   \n",
       "1 -104.987348  39.714316            3          311            speer        1   \n",
       "2 -104.988098  39.764528            6          612      five-points        1   \n",
       "3 -105.004665  39.739669            1          123     lincoln-park        1   \n",
       "4 -104.830661  39.795281            5          521        montbello        1   \n",
       "\n",
       "   IS_TRAFFIC VICTIM_COUNT             x             y  \n",
       "0           0            1  3.133789e+06  1.676471e+06  \n",
       "1           0            1  3.144221e+06  1.685476e+06  \n",
       "2           0            1  3.143907e+06  1.703765e+06  \n",
       "3           0            1  3.139299e+06  1.694684e+06  \n",
       "4           0            1  3.188083e+06  1.715255e+06  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 394736 records!\n"
     ]
    }
   ],
   "source": [
    "# Find all crime data CSV files matching the naming pattern\n",
    "csv_files = glob.glob(\"data/crime_split_*.csv\")\n",
    "\n",
    "# Read in all of the files and create Pandas dataframes from them\n",
    "split_dfs = [pd.read_csv(f) for f in csv_files]\n",
    "\n",
    "# Concatenate all the dataframes into a single one\n",
    "df = pd.concat(split_dfs, ignore_index=True)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "original_dataframe_shape = df.shape\n",
    "print(f'Loaded {original_dataframe_shape[0]} records!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Analysis\n",
    "The next thing to do is determine which columns will be useful for predictions and which columns need to be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime Dataset Columns:\n",
      " - OBJECTID\n",
      " - INCIDENT_ID\n",
      " - OFFENSE_ID\n",
      " - OFFENSE_CODE\n",
      " - OFFENSE_CODE_EXTENSION\n",
      " - OFFENSE_TYPE_ID\n",
      " - OFFENSE_CATEGORY_ID\n",
      " - FIRST_OCCURRENCE_DATE\n",
      " - LAST_OCCURRENCE_DATE\n",
      " - REPORTED_DATE\n",
      " - INCIDENT_ADDRESS\n",
      " - GEO_X\n",
      " - GEO_Y\n",
      " - GEO_LON\n",
      " - GEO_LAT\n",
      " - DISTRICT_ID\n",
      " - PRECINCT_ID\n",
      " - NEIGHBORHOOD_ID\n",
      " - IS_CRIME\n",
      " - IS_TRAFFIC\n",
      " - VICTIM_COUNT\n",
      " - x\n",
      " - y\n"
     ]
    }
   ],
   "source": [
    "print(\"Crime Dataset Columns:\")\n",
    "for column in df.columns:\n",
    "    print(f\" - {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Decisions\n",
    "| Column ID                 | Decision  | Explanation                                                                                        |\n",
    "|---------------------------|-----------|----------------------------------------------------------------------------------------------------|\n",
    "| `OBJECTID`                | Discarded | Related to the database how how the data is stored, identified, or queried. No predictive value.   |\n",
    "| `INCIDENT_ID`             | Discarded | Related to the database how how the data is stored, identified, or queried. No predictive value.   |\n",
    "| `OFFENSE_ID`              | Discarded | Related to the database how how the data is stored, identified, or queried. No predictive value.   |\n",
    "| `OFFENSE_CODE`            | Discarded | Related to the database how how the data is stored, identified, or queried. No predictive value.   |\n",
    "| `OFFENSE_CODE_EXTENSION`  | Discarded | Related to the database how how the data is stored, identified, or queried. No predictive value.   |\n",
    "| `OFFENSE_TYPE_ID`         | Kept      | This is part of the output (y-values). We will keep this column and it will be the subject of prediction by the classification model. |\n",
    "| `OFFENSE_CATEGORY_ID`     | Discarded | This is part of the output (y-values). This will be discarded in  |\n",
    "| `FIRST_OCCURRENCE_DATE`   | Kept      | Timestamp of the event, this will be kept and used for predictions made based on the time of day. |\n",
    "| `LAST_OCCURRENCE_DATE`    | Discarded | Undefined for many rows. We will discard this timestamp and rely on `FIRST_OCCURRENCE_DATE` instead.                              |\n",
    "| `REPORTED_DATE`           | Discarded | This is part of the output (y-values). Report time delta from occurrence varies incident to incident. No predictive value.                             |\n",
    "| `INCIDENT_ADDRESS`        | Discarded | Although address is useful, this column is not standardized with entries like \"2400 block\" rather than exact addresses in some cases. |\n",
    "| `GEO_X`                   | Discarded | Related to the mapping platform the data was posted on. No predictive value.                    |\n",
    "| `GEO_Y`                   | Discarded | Related to the mapping platform the data was posted on. No predictive value.                    |\n",
    "| `GEO_LON`                 | Kept      | Location of the crime, this will be kept and used to make predictions based on location. |\n",
    "| `GEO_LAT`                 | Kept      | Location of the crime, this will be kept and used to make predictions based on location. |\n",
    "| `DISTRICT_ID`             | Discarded | This is part of the output (y-values) and we have no way of querying. No predictive value.                    |\n",
    "| `PRECINCT_ID`             | Discarded | This is part of the output (y-values) and we have no way of querying. No predictive value.                             |\n",
    "| `NEIGHBORHOOD_ID`         | Discarded | Although neighborhood is useful, as some are \"rougher\" than others, we don't have a standard way of querying.                             |\n",
    "| `IS_CRIME`                | Discarded | This is part of the output (y-values). No predictive value.                             |\n",
    "| `IS_TRAFFIC`              | Discarded | This is part of the output (y-values). No predictive value.                             |\n",
    "| `VICTIM_COUNT`            | Discarded | This is part of the output (y-values). No predictive value.                             |\n",
    "| `x`                       | Discarded | Related to the mapping platform the data was posted on. No predictive value.                     |\n",
    "| `y`                       | Discarded | Related to the mapping platform the data was posted on. No predictive value.                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OFFENSE_TYPE_ID</th>\n",
       "      <th>FIRST_OCCURRENCE_DATE</th>\n",
       "      <th>GEO_LON</th>\n",
       "      <th>GEO_LAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307916</th>\n",
       "      <td>assault-simple</td>\n",
       "      <td>11/13/2022 12:37:00 AM</td>\n",
       "      <td>-104.987151</td>\n",
       "      <td>39.734529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328435</th>\n",
       "      <td>theft-other</td>\n",
       "      <td>2/1/2019 12:00:00 PM</td>\n",
       "      <td>-105.047890</td>\n",
       "      <td>39.697994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210093</th>\n",
       "      <td>robbery-street</td>\n",
       "      <td>7/28/2024 5:15:00 PM</td>\n",
       "      <td>-104.980822</td>\n",
       "      <td>39.740294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327155</th>\n",
       "      <td>theft-other</td>\n",
       "      <td>12/15/2020 8:00:00 PM</td>\n",
       "      <td>-104.848418</td>\n",
       "      <td>39.788114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41872</th>\n",
       "      <td>theft-of-motor-vehicle</td>\n",
       "      <td>5/26/2023 8:00:00 PM</td>\n",
       "      <td>-104.901779</td>\n",
       "      <td>39.773713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325102</th>\n",
       "      <td>theft-from-bldg</td>\n",
       "      <td>12/19/2019 12:45:00 PM</td>\n",
       "      <td>-104.674045</td>\n",
       "      <td>39.851926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290283</th>\n",
       "      <td>aggravated-assault</td>\n",
       "      <td>3/24/2022 5:12:00 PM</td>\n",
       "      <td>-105.007786</td>\n",
       "      <td>39.735287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280182</th>\n",
       "      <td>sex-off-fail-to-register</td>\n",
       "      <td>4/5/2024 4:00:00 PM</td>\n",
       "      <td>-104.992313</td>\n",
       "      <td>39.737154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109569</th>\n",
       "      <td>burglary-residence-by-force</td>\n",
       "      <td>5/9/2019 12:10:00 PM</td>\n",
       "      <td>-104.959866</td>\n",
       "      <td>39.729320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184904</th>\n",
       "      <td>disturbing-the-peace</td>\n",
       "      <td>3/25/2021 4:45:00 PM</td>\n",
       "      <td>-104.993085</td>\n",
       "      <td>39.745860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159768</th>\n",
       "      <td>robbery-business</td>\n",
       "      <td>12/5/2021 9:28:00 PM</td>\n",
       "      <td>-105.052614</td>\n",
       "      <td>39.732877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341093</th>\n",
       "      <td>theft-of-services</td>\n",
       "      <td>6/21/2022 10:45:00 PM</td>\n",
       "      <td>-104.667270</td>\n",
       "      <td>39.850723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385774</th>\n",
       "      <td>assault-simple</td>\n",
       "      <td>1/17/2023 10:59:00 AM</td>\n",
       "      <td>-104.987087</td>\n",
       "      <td>39.755448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361122</th>\n",
       "      <td>theft-of-motor-vehicle</td>\n",
       "      <td>1/18/2023 3:00:00 PM</td>\n",
       "      <td>-104.988390</td>\n",
       "      <td>39.715235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370747</th>\n",
       "      <td>violation-of-restraining-order</td>\n",
       "      <td>11/8/2023 4:46:00 AM</td>\n",
       "      <td>-105.047793</td>\n",
       "      <td>39.692693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88223</th>\n",
       "      <td>theft-parts-from-vehicle</td>\n",
       "      <td>12/24/2019 5:15:00 PM</td>\n",
       "      <td>-105.024455</td>\n",
       "      <td>39.694478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191199</th>\n",
       "      <td>criminal-trespassing</td>\n",
       "      <td>4/15/2021 9:15:00 PM</td>\n",
       "      <td>-105.001101</td>\n",
       "      <td>39.753916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70892</th>\n",
       "      <td>criminal-mischief-other</td>\n",
       "      <td>6/4/2022 12:01:00 AM</td>\n",
       "      <td>-104.985574</td>\n",
       "      <td>39.765810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231052</th>\n",
       "      <td>theft-of-motor-vehicle</td>\n",
       "      <td>3/5/2021 3:20:00 AM</td>\n",
       "      <td>-105.050390</td>\n",
       "      <td>39.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111830</th>\n",
       "      <td>liquor-possession</td>\n",
       "      <td>5/6/2021 10:25:00 AM</td>\n",
       "      <td>-104.973623</td>\n",
       "      <td>39.739957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       OFFENSE_TYPE_ID   FIRST_OCCURRENCE_DATE     GEO_LON  \\\n",
       "307916                  assault-simple  11/13/2022 12:37:00 AM -104.987151   \n",
       "328435                     theft-other    2/1/2019 12:00:00 PM -105.047890   \n",
       "210093                  robbery-street    7/28/2024 5:15:00 PM -104.980822   \n",
       "327155                     theft-other   12/15/2020 8:00:00 PM -104.848418   \n",
       "41872           theft-of-motor-vehicle    5/26/2023 8:00:00 PM -104.901779   \n",
       "325102                 theft-from-bldg  12/19/2019 12:45:00 PM -104.674045   \n",
       "290283              aggravated-assault    3/24/2022 5:12:00 PM -105.007786   \n",
       "280182        sex-off-fail-to-register     4/5/2024 4:00:00 PM -104.992313   \n",
       "109569     burglary-residence-by-force    5/9/2019 12:10:00 PM -104.959866   \n",
       "184904            disturbing-the-peace    3/25/2021 4:45:00 PM -104.993085   \n",
       "159768                robbery-business    12/5/2021 9:28:00 PM -105.052614   \n",
       "341093               theft-of-services   6/21/2022 10:45:00 PM -104.667270   \n",
       "385774                  assault-simple   1/17/2023 10:59:00 AM -104.987087   \n",
       "361122          theft-of-motor-vehicle    1/18/2023 3:00:00 PM -104.988390   \n",
       "370747  violation-of-restraining-order    11/8/2023 4:46:00 AM -105.047793   \n",
       "88223         theft-parts-from-vehicle   12/24/2019 5:15:00 PM -105.024455   \n",
       "191199            criminal-trespassing    4/15/2021 9:15:00 PM -105.001101   \n",
       "70892          criminal-mischief-other    6/4/2022 12:01:00 AM -104.985574   \n",
       "231052          theft-of-motor-vehicle     3/5/2021 3:20:00 AM -105.050390   \n",
       "111830               liquor-possession    5/6/2021 10:25:00 AM -104.973623   \n",
       "\n",
       "          GEO_LAT  \n",
       "307916  39.734529  \n",
       "328435  39.697994  \n",
       "210093  39.740294  \n",
       "327155  39.788114  \n",
       "41872   39.773713  \n",
       "325102  39.851926  \n",
       "290283  39.735287  \n",
       "280182  39.737154  \n",
       "109569  39.729320  \n",
       "184904  39.745860  \n",
       "159768  39.732877  \n",
       "341093  39.850723  \n",
       "385774  39.755448  \n",
       "361122  39.715235  \n",
       "370747  39.692693  \n",
       "88223   39.694478  \n",
       "191199  39.753916  \n",
       "70892   39.765810  \n",
       "231052  39.695462  \n",
       "111830  39.739957  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep only the columns that we want\n",
    "df = df[['OFFENSE_TYPE_ID','FIRST_OCCURRENCE_DATE','GEO_LON','GEO_LAT']]\n",
    "\n",
    "# Displaying a random sample of the data to confirm what it looks like\n",
    "display(df.sample(n=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Binning\n",
    "In this analysis, time binning is used to capture the cyclical and seasonal patterns inherent in criminal behavior. Crime often follows temporal trends—certain types of incidents may be more likely to occur at specific times of day, days of the week, or during particular seasons. For example, incidents related to nightlife might peak during late evening hours, while other crimes may correlate with daily commuter traffic or specific days like weekends. By extracting temporal features like `YEAR`, `DAY_OF_YEAR`, `DAY_OF_WEEK`, and a continuous `TIME` (floating-point hour), we enable the model to identify these patterns and trends. This granular binning enhances the predictive power of the model, allowing it to detect not only broad time-based patterns but also more subtle nuances in how crime evolves throughout the day, week, and year.\n",
    "\n",
    "#### Temporal Features Breakdown\n",
    "* `YEAR`: Provides a clear yearly context, which can be crucial for identifying long-term trends or annual shifts in patterns.\n",
    "* `TIME` (floating point): Offers a precise measure of the time of day, allowing models to pick up on subtle temporal shifts within a 24-hour cycle.\n",
    "* `DAY_OF_YEAR`: Captures the day within the year, useful for identifying seasonal patterns or trends that recur annually.\n",
    "* `DAY_OF_WEEK`: Encodes weekly cycles, helping the model detect patterns associated with specific weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OFFENSE_TYPE_ID</th>\n",
       "      <th>GEO_LON</th>\n",
       "      <th>GEO_LAT</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>DAY_OF_YEAR</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106892</th>\n",
       "      <td>burglary-residence-by-force</td>\n",
       "      <td>-104.981270</td>\n",
       "      <td>39.746000</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112631</th>\n",
       "      <td>police-resisting-arrest</td>\n",
       "      <td>-104.890420</td>\n",
       "      <td>39.740404</td>\n",
       "      <td>2021</td>\n",
       "      <td>13</td>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103240</th>\n",
       "      <td>threats-to-injure</td>\n",
       "      <td>-104.934029</td>\n",
       "      <td>39.732320</td>\n",
       "      <td>2019</td>\n",
       "      <td>22</td>\n",
       "      <td>346</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107906</th>\n",
       "      <td>burglary-residence-by-force</td>\n",
       "      <td>-104.909892</td>\n",
       "      <td>39.698283</td>\n",
       "      <td>2021</td>\n",
       "      <td>15</td>\n",
       "      <td>274</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328136</th>\n",
       "      <td>theft-other</td>\n",
       "      <td>-105.034096</td>\n",
       "      <td>39.744742</td>\n",
       "      <td>2020</td>\n",
       "      <td>16</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    OFFENSE_TYPE_ID     GEO_LON    GEO_LAT  YEAR  HOUR  \\\n",
       "106892  burglary-residence-by-force -104.981270  39.746000  2021     2   \n",
       "112631      police-resisting-arrest -104.890420  39.740404  2021    13   \n",
       "103240            threats-to-injure -104.934029  39.732320  2019    22   \n",
       "107906  burglary-residence-by-force -104.909892  39.698283  2021    15   \n",
       "328136                  theft-other -105.034096  39.744742  2020    16   \n",
       "\n",
       "        DAY_OF_YEAR  DAY_OF_WEEK  \n",
       "106892           26            1  \n",
       "112631          174            2  \n",
       "103240          346            3  \n",
       "107906          274            4  \n",
       "328136          339            4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert FIRST_OCCURRENCE_DATE to Pandas date time format\n",
    "df['FIRST_OCCURRENCE_DATE'] = pd.to_datetime(df['FIRST_OCCURRENCE_DATE'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Extract date and time components\n",
    "df['YEAR'] = df['FIRST_OCCURRENCE_DATE'].dt.year\n",
    "# df['MONTH'] = df['FIRST_OCCURRENCE_DATE'].dt.month -- Removed in favor of DAY_OF_YEAR\n",
    "# df['DAY_OF_MONTH'] = df['FIRST_OCCURRENCE_DATE'].dt.day -- Removed in favor of DAY_OF_YEAR\n",
    "# df['TIME'] = (\n",
    "#     df['FIRST_OCCURRENCE_DATE'].dt.hour + \n",
    "#     df['FIRST_OCCURRENCE_DATE'].dt.minute / 60 + \n",
    "#     df['FIRST_OCCURRENCE_DATE'].dt.second / 3600\n",
    "# ) -- No longer doing floating point hour\n",
    "df['HOUR'] = df['FIRST_OCCURRENCE_DATE'].dt.hour\n",
    "df['DAY_OF_YEAR'] = df['FIRST_OCCURRENCE_DATE'].dt.dayofyear\n",
    "df['DAY_OF_WEEK'] = df['FIRST_OCCURRENCE_DATE'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Finally, dropping the FIRST_OCCURRENCE_DATE column\n",
    "df.drop(columns=['FIRST_OCCURRENCE_DATE'], inplace=True)\n",
    "\n",
    "# Displaying a random sample of the data to confirm what it looks like\n",
    "display(df.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Slicing & Binning \n",
    "In this step, we convert latitude and longitude into a 15-meter by 15-meter grid system using a projected coordinate system (UTM). This method ensures that each crime location is assigned to a unique, precise grid cell, facilitating accurate spatial analysis.\n",
    "\n",
    "See <https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system>\n",
    "\n",
    "#### Why This is Better Than Using Raw Latitude and Longitude\n",
    "\n",
    "Using UTM coordinates overcomes the limitations of raw latitude and longitude, which vary in distance depending on location. UTM coordinates provide uniform distance measurements in meters, improving precision and making it easier to define consistent grid cells. This approach also simplifies spatial analysis by grouping data into fixed-size cells, making patterns like crime hotspots easier to identify and computationally more efficient to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size before removal: 394736 items\n",
      "Dataframe size after removal: 394475 items\n",
      "--> 261 items removed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OFFENSE_TYPE_ID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>DAY_OF_YEAR</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>X_BLOCK</th>\n",
       "      <th>Y_BLOCK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56492</th>\n",
       "      <td>theft-items-from-vehicle</td>\n",
       "      <td>2021</td>\n",
       "      <td>21</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>35149</td>\n",
       "      <td>294034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160448</th>\n",
       "      <td>robbery-street</td>\n",
       "      <td>2019</td>\n",
       "      <td>14</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>33414</td>\n",
       "      <td>293344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38856</th>\n",
       "      <td>drug-pcs-other-drug</td>\n",
       "      <td>2021</td>\n",
       "      <td>16</td>\n",
       "      <td>335</td>\n",
       "      <td>2</td>\n",
       "      <td>33191</td>\n",
       "      <td>293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85231</th>\n",
       "      <td>theft-parts-from-vehicle</td>\n",
       "      <td>2021</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>33713</td>\n",
       "      <td>292982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363235</th>\n",
       "      <td>criminal-mischief-other</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>34642</td>\n",
       "      <td>293583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 OFFENSE_TYPE_ID  YEAR  HOUR  DAY_OF_YEAR  DAY_OF_WEEK  \\\n",
       "56492   theft-items-from-vehicle  2021    21          171            6   \n",
       "160448            robbery-street  2019    14          266            0   \n",
       "38856        drug-pcs-other-drug  2021    16          335            2   \n",
       "85231   theft-parts-from-vehicle  2021    14           23            5   \n",
       "363235   criminal-mischief-other  2023     2           28            5   \n",
       "\n",
       "        X_BLOCK  Y_BLOCK  \n",
       "56492     35149   294034  \n",
       "160448    33414   293344  \n",
       "38856     33191   293600  \n",
       "85231     33713   292982  \n",
       "363235    34642   293583  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GRID_SIZE = 15 # 15m x 15m grid cells\n",
    "\n",
    "# Initialize UTM projection (assuming UTM Zone 13N for Denver)\n",
    "#   https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system\n",
    "#   https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system#/media/File:Universal_Transverse_Mercator_zones.svg\n",
    "utm_proj = pyproj.Proj(proj=\"utm\", zone=13, datum=\"WGS84\")\n",
    "\n",
    "def get_grid_block(lat, lon):\n",
    "    try:\n",
    "        # Convert latitude and longitude to UTM coordinates (X, Y)\n",
    "        x, y = utm_proj(lon, lat)  # Correct order: lon, lat\n",
    "        \n",
    "        # Calculate the X and Y blocks\n",
    "        x_block = int(x // GRID_SIZE)\n",
    "        y_block = int(y // GRID_SIZE)\n",
    "        \n",
    "        return x_block, y_block\n",
    "    except:\n",
    "        # If there is an issue parsing, return -1\n",
    "        return -1, -1\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df[['X_BLOCK', 'Y_BLOCK']] = df.apply(lambda row: get_grid_block(row['GEO_LAT'], row['GEO_LON']), axis=1, result_type='expand')\n",
    "\n",
    "# Drop rows where X_BLOCK or Y_BLOCK is -1\n",
    "df = df[(df['X_BLOCK'] != -1) & (df['Y_BLOCK'] != -1)]\n",
    "\n",
    "# Note change in size\n",
    "post_null_geo_block_removal_dataframe_shape = df.shape\n",
    "print(f'Dataframe size before removal: {original_dataframe_shape[0]} items')\n",
    "print(f'Dataframe size after removal: {post_null_geo_block_removal_dataframe_shape[0]} items')\n",
    "print(f'--> {original_dataframe_shape[0] - post_null_geo_block_removal_dataframe_shape[0]} items removed!')\n",
    "\n",
    "# Finally, dropping the GEO_LAT and GEO_LON columns since we have replaced them with X_BLOCK and Y_BLOCK\n",
    "df.drop(columns=['GEO_LAT'], inplace=True)\n",
    "df.drop(columns=['GEO_LON'], inplace=True)\n",
    "\n",
    "# Displaying a random sample of the data to confirm what it looks like\n",
    "display(df.sample(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are processing the geographical data to assign each crime incident to a specific grid cell based on its latitude and longitude. We achieve this by converting the coordinates to UTM projection, which provides more accurate distance-based calculations than traditional latitude and longitude. For each point, we calculate the corresponding grid cell by dividing the UTM coordinates by the grid size (15 meters) and then storing the results as `X_BLOCK` and `Y_BLOCK`.\n",
    "\n",
    "However, if any coordinates cannot be processed correctly (due to projection issues or invalid data), we assign -1 as a placeholder. After applying this transformation, we remove any rows where either the `X_BLOCK` or `Y_BLOCK` is -1, indicating invalid grid assignments. The sizes of the DataFrame before and after this removal are printed to highlight the impact of cleaning the data. This step ensures that only valid data points are included for further analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Location Outliers\n",
    "When exporting the data as a CSV file, it can be observed that some of the rows have really low or high X-block values. These are likely location mis-inputs.\n",
    "\n",
    "We can drop these rows by calculating the interquartile range range for the `X_BLOCK` and `Y_BLOCK` columns and dropping the rows that are outliers.\n",
    "\n",
    "What I have done below is not the interquartile range. I only want to drop extreme outliers, so I dropped the values that fall outside the middle 98% of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before location outlier removal: 394475 records.\n",
      "Size after location outlier removal: 394434 records.\n",
      "--> 41 records removed!\n"
     ]
    }
   ],
   "source": [
    "df_shape_before_location_outlier_drop = df.shape\n",
    "\n",
    "Q1 = df['X_BLOCK'].quantile(0.01)\n",
    "Q3 = df['X_BLOCK'].quantile(0.99)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df.loc[(df['X_BLOCK'] >= lower_bound) & (df['X_BLOCK'] <= upper_bound)]\n",
    "\n",
    "# Calculate and display rows removed\n",
    "df_shape_after_location_outlier_drop = df.shape\n",
    "print(f'Size before location outlier removal: {df_shape_before_location_outlier_drop[0]} records.')\n",
    "print(f'Size after location outlier removal: {df_shape_after_location_outlier_drop[0]} records.')\n",
    "print(f'--> {df_shape_before_location_outlier_drop[0] - df_shape_after_location_outlier_drop[0]} records removed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding in OpenStreetMap Features\n",
    "In this section, we aim to enrich our dataset with spatial features from OpenStreetMap (OSM). By querying OSM data for geographic features based on the centroids of the X and Y grid blocks (calculated from latitude and longitude), we can incorporate valuable context such as the proximity to roads, points of interest, or land use types. This spatial data enhances our analysis by providing a richer understanding of the environment in which the crimes occur, potentially improving predictive modeling. By leveraging OpenStreetMap data, we can gather additional information about the geographical characteristics of each grid cell, allowing for more nuanced predictions based on spatial context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "# Import osmium to parse the OSM files\n",
    "import osmium\n",
    "\n",
    "# Re-define the UTM projections from earlier\n",
    "utm_proj = pyproj.Proj(proj=\"utm\", zone=13, datum=\"WGS84\")\n",
    "latlon_proj = pyproj.Proj(proj=\"latlong\", datum=\"WGS84\")\n",
    "\n",
    "# Define tranformers\n",
    "latlon_to_utm_transformer = pyproj.Transformer.from_proj(latlon_proj, utm_proj)\n",
    "utm_to_latlon_transformer = pyproj.Transformer.from_proj(utm_proj, latlon_proj)\n",
    "\n",
    "'''\n",
    "    Generate a Grid of Map Blocks/Cells from predefined corners.\n",
    "'''\n",
    "def gen_blocks_from_coordinates(southwest_corner, northeast_corner, database_path):\n",
    "    # Bounding corners\n",
    "    def dec_coordinates(str_coordinates):\n",
    "        parts = str_coordinates.split(\",\")\n",
    "        \n",
    "        # Extract latitude from the first part and longitude from the second part\n",
    "        lat = float(re.findall(r\"\\d+\\.\\d+\", parts[0])[0])\n",
    "        lon = float(re.findall(r\"\\d+\\.\\d+\", parts[1])[0])\n",
    "        \n",
    "        # Adjust sign based on N/S and E/W indicators\n",
    "        if \"S\" in parts[0]:\n",
    "            lat = -lat\n",
    "        if \"W\" in parts[1]:\n",
    "            lon = -lon\n",
    "            \n",
    "        return lon, lat  # Return (longitude, latitude)\n",
    "\n",
    "    southwest_lon, southwest_lat = dec_coordinates(southwest_corner)\n",
    "    northeast_lon, northeast_lat = dec_coordinates(northeast_corner)\n",
    "\n",
    "    # Define block size in meters (15 meters as mentioned)\n",
    "    block_size = 15\n",
    "\n",
    "    # Create a transformer for converting lat/lon to UTM\n",
    "    latlon_to_utm_transformer = pyproj.Transformer.from_proj(latlon_proj, utm_proj)\n",
    "\n",
    "    # Convert southwest and northeast corners to UTM\n",
    "    southwest_x, southwest_y = latlon_to_utm_transformer.transform(southwest_lon, southwest_lat)\n",
    "    northeast_x, northeast_y = latlon_to_utm_transformer.transform(northeast_lon, northeast_lat)\n",
    "\n",
    "    # Calculate the area width and height in meters\n",
    "    area_width = northeast_x - southwest_x\n",
    "    area_height = northeast_y - southwest_y\n",
    "\n",
    "    # Calculate the number of blocks in the area\n",
    "    num_x_blocks = int(area_width // block_size)\n",
    "    num_y_blocks = int(area_height // block_size)\n",
    "\n",
    "    # Create SQLite database and table\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create a table to store the map cells\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS blocks (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            x_block INTEGER,\n",
    "            y_block INTEGER,\n",
    "            cent_lat REAL,\n",
    "            cent_lon REAL\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Function to calculate the centroid of a block (in UTM)\n",
    "    def get_centroid(x_block, y_block, grid_size=15):\n",
    "        # Create a transformer for converting UTM to lat/lon\n",
    "        utm_to_latlon_transformer = pyproj.Transformer.from_proj(utm_proj, latlon_proj)\n",
    "        \n",
    "        # Get the bottom-left corner coordinates in UTM\n",
    "        x_origin, y_origin = x_block * grid_size, y_block * grid_size\n",
    "        \n",
    "        # Calculate the centroid by moving half the grid size in both directions\n",
    "        x_centroid = x_origin + grid_size / 2\n",
    "        y_centroid = y_origin + grid_size / 2\n",
    "        \n",
    "        # Convert UTM coordinates to latitude and longitude\n",
    "        lon, lat = utm_to_latlon_transformer.transform(x_centroid, y_centroid)\n",
    "        \n",
    "        return lat, lon\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    total_blocks = num_x_blocks * num_y_blocks\n",
    "\n",
    "    # Set the batch size\n",
    "    batch_size = 100000\n",
    "    batch_data = []\n",
    "\n",
    "    # Init the progress bar\n",
    "    progress_bar = tqdm(total=total_blocks, desc=\"Identifying Block Centroids\", position=0, ncols=120)\n",
    "\n",
    "    # Loop through each block and calculate the centroid\n",
    "    for y_block in range(num_y_blocks):\n",
    "        for x_block in range(num_x_blocks):\n",
    "            # Calculate the centroid for the current block\n",
    "            lat, lon = get_centroid(x_block, y_block, block_size)\n",
    "            \n",
    "            # Append data to batch\n",
    "            batch_data.append((x_block, y_block, lat, lon))\n",
    "            \n",
    "            # If batch size is reached, execute the batch insert\n",
    "            if len(batch_data) >= batch_size:\n",
    "                cursor.executemany('''\n",
    "                    INSERT INTO blocks (x_block, y_block, cent_lat, cent_lon)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                ''', batch_data)\n",
    "                \n",
    "                # Manually update the progress bar after each block\n",
    "                progress_bar.update(len(batch_data))\n",
    "                \n",
    "                conn.commit()  # Commit after each batch\n",
    "                batch_data = []  # Clear the batch data\n",
    "\n",
    "    # Insert any remaining data after the loop\n",
    "    if batch_data:\n",
    "        cursor.executemany('''\n",
    "            INSERT INTO blocks (x_block, y_block, cent_lat, cent_lon)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', batch_data)\n",
    "\n",
    "        progress_bar.update(len(batch_data))\n",
    "\n",
    "        conn.commit()  # Final commit for remaining data\n",
    "\n",
    "    # Close the progress bar once finished\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Close the SQLite connection\n",
    "    conn.close()\n",
    "\n",
    "'''\n",
    "    Generate a map based on the boundaries of the available crime location data, plus a margin\n",
    "'''\n",
    "def gen_map_from_crime_location_data_with_margin(crime_df, margin, database_path):\n",
    "    # Get the limits of the locations saved within the data frame\n",
    "    x_max = crime_df['X_BLOCK'].max()\n",
    "    x_min = crime_df['X_BLOCK'].min()\n",
    "    y_max = crime_df['Y_BLOCK'].max()\n",
    "    y_min = crime_df['Y_BLOCK'].min()\n",
    "\n",
    "    # Calculate the limits with the \n",
    "    x_max_margin = x_max + margin\n",
    "    x_min_margin = x_min - margin\n",
    "    y_max_margin = y_max + margin\n",
    "    y_min_margin = y_min - margin\n",
    "\n",
    "    # Print the map size\n",
    "    print(f\"Map Size Before Margin (in blocks): Upper Left Corner: ({x_min}, {y_max}), Lower Right Corner: ({x_max}, {y_min})\")\n",
    "    print(f\"Map Size After Margin (in blocks): Upper Left Corner: ({x_min_margin}, {y_max_margin}), Lower Right Corner: ({x_max_margin}, {y_min_margin})\")\n",
    "\n",
    "    # Function to get the edges of the box in lon/lat\n",
    "    def block_to_latlon(x, y):\n",
    "        # Project from UTM to latlon\n",
    "        transformer = pyproj.Transformer.from_crs(f\"epsg:32613\", \"epsg:4326\", always_xy=True)\n",
    "        return transformer.transform(x * GRID_SIZE, y * GRID_SIZE)\n",
    "\n",
    "    min_longitude, min_latitude = block_to_latlon(x_min_margin, y_min_margin)\n",
    "    max_longitude, max_latitude = block_to_latlon(x_max_margin, y_max_margin)\n",
    "    print(f\"    (lon, lat): ({min_longitude},{min_latitude}) ({max_longitude},{max_latitude})\")\n",
    "\n",
    "    # Loop through the each possible block in the map, including the margins\n",
    "    total_blocks = (x_max_margin - x_min_margin) * (y_max_margin - y_min_margin)\n",
    "    print(f'--> Total map grid cells: {total_blocks}')\n",
    "    \n",
    "    # Create SQLite database and table\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create a table to store the map cells\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS blocks (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            x INTEGER,\n",
    "            y INTEGER,\n",
    "            c_lat REAL,\n",
    "            c_lon REAL,\n",
    "            tags TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Set the batch size\n",
    "    batch_size = 1\n",
    "    batch_data = []\n",
    "    \n",
    "    # Init the progress bar\n",
    "    progress_bar = tqdm(total=total_blocks, desc=\"Building Map\", position=0, ncols=120)\n",
    "\n",
    "    for x in range(x_min_margin, x_max_margin + 1):\n",
    "        for y in range(y_min_margin, y_max_margin + 1):\n",
    "            # Get important latitudes and longitudes\n",
    "            center_lon, center_lat = block_to_latlon(x + (GRID_SIZE / 2), y - (GRID_SIZE / 2))\n",
    "\n",
    "            tags = []\n",
    "            \n",
    "            for obj in osmium.FileProcessor('../denver_filtered.osm.pbf', osmium.osm.NODE):\n",
    "                if obj.tags:\n",
    "                    tags += obj,tags\n",
    "                try:\n",
    "                    if osmium.geom.haversine_distance(osmium.osm.Location(center_lon, center_lat), obj.location) < (GRID_SIZE * 1.5):\n",
    "                        continue\n",
    "                except:\n",
    "                    print(obj)\n",
    "            \n",
    "            print(tags)\n",
    "            \n",
    "            batch_data.append((\n",
    "                x,          #\n",
    "                y,          #\n",
    "                center_lon, #\n",
    "                center_lat, #\n",
    "                str(tags)        #\n",
    "            ))\n",
    "\n",
    "            # If batch size is reached, execute the batch insert\n",
    "            if len(batch_data) >= batch_size:\n",
    "                cursor.executemany('''\n",
    "                    INSERT INTO blocks (x, y, c_lat, c_lon, tags)\n",
    "                    VALUES (?, ?, ?, ?, ?)\n",
    "                ''', batch_data)\n",
    "                \n",
    "                # Manually update the progress bar after each block\n",
    "                progress_bar.update(len(batch_data))\n",
    "                \n",
    "                conn.commit()  # Commit after each batch\n",
    "                batch_data = []  # Clear the batch data\n",
    "            \n",
    "    # Insert any remaining data after the loop\n",
    "    if batch_data:\n",
    "        cursor.executemany('''\n",
    "            INSERT INTO blocks (x, y, c_lat, c_lon, tags)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        ''', batch_data)\n",
    "\n",
    "        progress_bar.update(len(batch_data))\n",
    "\n",
    "        conn.commit()  # Final commit for remaining data\n",
    "\n",
    "    # Close the SQLite connection\n",
    "    conn.close()    \n",
    "    \n",
    "    # Close the progress bar once finished\n",
    "    progress_bar.close()\n",
    "\n",
    "# Old method was just a fixed area\n",
    "southwest_corner = \"39.53440° N, 105.24364° W\"\n",
    "northeast_corner = \"40.03841° N, 104.58877° W\"\n",
    "database_path = 'map_grid.db'\n",
    "#gen_blocks_from_coordinates(southwest_corner, northeast_corner, database_path)\n",
    "\n",
    "# gen_map_from_crime_location_data_with_margin(df, 400, 'map.db')\n",
    "# -- Skipping... see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "12676781 * 17 / 60 / 60 / 24 / 365 = 6.8336275051 \\text{ years to process the OSM data}\n",
    "$$\n",
    "For now, I will set aside the extraction of OpenStreetMap (OSM) features due to the excessive processing time — approximately 6.83 years — required to handle the data with my current approach. The complexity of querying and filtering OSM data, even with optimization attempts, has proven to be a significant bottleneck. As a result, I will proceed with exploratory data analysis (EDA) using the existing crime dataset without incorporating OSM features. This will allow me to focus on other aspects of the analysis and revisit the OSM integration if a more efficient method becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the initial dataframe\n",
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime Type Model (Gradient Boosting Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Distribution:\n",
      " OFFENSE_TYPE_ID\n",
      "150    47402\n",
      "148    32539\n",
      "35     22177\n",
      "154    20602\n",
      "153    18918\n",
      "       ...  \n",
      "92         2\n",
      "177        2\n",
      "68         2\n",
      "90         2\n",
      "86         2\n",
      "Name: count, Length: 171, dtype: int64\n",
      "Test Class Distribution:\n",
      " OFFENSE_TYPE_ID\n",
      "150    11851\n",
      "148     8135\n",
      "35      5544\n",
      "154     5151\n",
      "153     4730\n",
      "       ...  \n",
      "113        1\n",
      "86         1\n",
      "123        1\n",
      "109        1\n",
      "94         1\n",
      "Name: count, Length: 164, dtype: int64\n",
      "Accuracy: 0.14656593058337347\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        11\n",
      "           1       0.03      0.03      0.03        61\n",
      "           2       0.01      0.01      0.01       187\n",
      "           3       0.10      0.01      0.02      1442\n",
      "           4       0.01      0.00      0.00       656\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00        28\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00        26\n",
      "           9       0.01      0.02      0.01        64\n",
      "          10       0.00      0.00      0.00         4\n",
      "          11       0.00      0.00      0.00        30\n",
      "          12       0.00      0.00      0.00        47\n",
      "          13       0.00      0.00      0.00         5\n",
      "          14       0.02      0.00      0.01       981\n",
      "          15       0.03      0.02      0.02       149\n",
      "          16       0.22      0.02      0.03      2916\n",
      "          18       0.00      0.00      0.00        39\n",
      "          19       0.00      0.00      0.00         8\n",
      "          20       0.00      0.00      0.00        23\n",
      "          21       0.01      0.02      0.01        65\n",
      "          22       0.01      0.01      0.01       127\n",
      "          23       0.00      0.00      0.00        29\n",
      "          24       0.28      0.06      0.10      1845\n",
      "          25       0.03      0.01      0.01       505\n",
      "          26       0.02      0.02      0.02        88\n",
      "          27       0.00      0.00      0.00        76\n",
      "          28       0.01      0.00      0.00      1240\n",
      "          29       0.06      0.00      0.01      1699\n",
      "          30       0.00      0.00      0.00        25\n",
      "          31       0.04      0.11      0.06        28\n",
      "          32       0.11      0.18      0.14        79\n",
      "          33       0.01      0.11      0.02         9\n",
      "          34       0.19      0.05      0.08       497\n",
      "          35       0.13      0.01      0.02      5544\n",
      "          36       0.08      0.00      0.01      3310\n",
      "          37       0.22      0.10      0.13      2918\n",
      "          38       0.16      0.24      0.19        86\n",
      "          39       0.00      0.00      0.00         9\n",
      "          40       0.06      0.01      0.02       726\n",
      "          42       0.00      0.00      0.00         2\n",
      "          43       0.00      0.00      0.00         1\n",
      "          44       0.03      0.02      0.02       166\n",
      "          45       0.15      0.16      0.16       202\n",
      "          46       0.00      0.00      0.00         4\n",
      "          47       0.00      0.00      0.00        18\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.00      0.00      0.00         9\n",
      "          50       0.00      0.00      0.00        10\n",
      "          51       0.02      0.01      0.01       140\n",
      "          52       0.00      0.00      0.00        54\n",
      "          53       0.01      0.02      0.02        43\n",
      "          54       0.02      0.09      0.03        11\n",
      "          55       0.00      0.00      0.00        37\n",
      "          56       0.00      0.00      0.00        28\n",
      "          57       0.10      0.02      0.03       521\n",
      "          58       0.02      0.02      0.02       194\n",
      "          59       0.00      0.00      0.00         6\n",
      "          60       0.02      0.03      0.02        67\n",
      "          61       0.07      0.05      0.06       195\n",
      "          62       0.05      0.02      0.03       833\n",
      "          63       0.33      0.29      0.31      1081\n",
      "          64       0.00      0.00      0.00        23\n",
      "          65       0.00      0.00      0.00        33\n",
      "          67       0.00      0.00      0.00        11\n",
      "          68       0.00      0.00      0.00         0\n",
      "          69       0.00      0.00      0.00         4\n",
      "          70       0.00      0.00      0.00         4\n",
      "          71       0.01      0.01      0.01        70\n",
      "          72       0.00      0.00      0.00       118\n",
      "          73       0.00      0.00      0.00        15\n",
      "          74       0.00      0.00      0.00       152\n",
      "          75       0.00      0.00      0.00        11\n",
      "          76       0.00      0.00      0.00        41\n",
      "          77       0.00      0.00      0.00         5\n",
      "          78       0.00      0.00      0.00        17\n",
      "          79       0.00      0.00      0.00        16\n",
      "          80       0.00      0.00      0.00       328\n",
      "          81       0.02      0.01      0.01       281\n",
      "          82       0.02      0.02      0.02       112\n",
      "          83       0.00      0.00      0.00         2\n",
      "          84       0.00      0.00      0.00        26\n",
      "          85       0.00      0.00      0.00         0\n",
      "          86       0.00      0.00      0.00         1\n",
      "          87       0.02      0.00      0.01       289\n",
      "          88       0.02      0.01      0.01       135\n",
      "          89       0.00      0.00      0.00        81\n",
      "          90       0.00      0.00      0.00         0\n",
      "          91       0.00      0.00      0.00         6\n",
      "          92       0.00      0.00      0.00         0\n",
      "          93       0.01      0.01      0.01        76\n",
      "          94       0.00      0.00      0.00         1\n",
      "          96       0.00      0.00      0.00        39\n",
      "          97       0.00      0.00      0.00         9\n",
      "          98       0.00      0.00      0.00       142\n",
      "          99       0.00      0.00      0.00        22\n",
      "         100       0.00      0.00      0.00        47\n",
      "         101       0.00      0.00      0.00        54\n",
      "         102       0.00      0.00      0.00         1\n",
      "         103       0.35      0.32      0.33       313\n",
      "         104       0.08      0.13      0.10        46\n",
      "         105       0.00      0.00      0.00         9\n",
      "         106       0.00      0.00      0.00         1\n",
      "         107       0.02      0.01      0.01      1047\n",
      "         108       0.00      0.00      0.00         0\n",
      "         109       0.00      0.00      0.00         1\n",
      "         110       0.00      0.00      0.00         4\n",
      "         111       0.00      0.00      0.00        52\n",
      "         112       0.00      0.00      0.00         9\n",
      "         113       0.00      0.00      0.00         1\n",
      "         114       0.00      0.00      0.00         5\n",
      "         115       0.10      0.12      0.11        58\n",
      "         116       0.01      0.00      0.01       281\n",
      "         117       0.04      0.02      0.03       237\n",
      "         118       0.00      0.00      0.00        10\n",
      "         119       0.02      0.04      0.02        26\n",
      "         120       0.00      0.00      0.00       152\n",
      "         121       0.00      0.00      0.00         1\n",
      "         122       0.00      0.00      0.00        27\n",
      "         123       0.00      0.00      0.00         1\n",
      "         124       0.25      0.36      0.29       107\n",
      "         125       0.00      0.00      0.00         2\n",
      "         126       0.03      0.03      0.03        86\n",
      "         127       0.23      0.10      0.14       591\n",
      "         128       0.00      0.00      0.00        64\n",
      "         129       0.00      0.00      0.00        23\n",
      "         131       0.01      0.05      0.02        42\n",
      "         132       0.03      0.01      0.02       411\n",
      "         133       0.00      0.00      0.00       197\n",
      "         134       0.00      0.00      0.00        44\n",
      "         135       0.00      0.00      0.00        76\n",
      "         136       0.05      0.01      0.02       641\n",
      "         137       0.66      0.78      0.72       263\n",
      "         138       0.10      0.30      0.15        33\n",
      "         139       0.00      0.00      0.00        40\n",
      "         140       0.12      0.02      0.03      1889\n",
      "         141       0.00      0.00      0.00         9\n",
      "         142       0.00      0.00      0.00        20\n",
      "         143       0.30      0.33      0.31       150\n",
      "         144       0.05      0.02      0.03      1700\n",
      "         145       0.02      0.01      0.02       149\n",
      "         147       0.00      0.00      0.00         6\n",
      "         148       0.19      0.10      0.13      8135\n",
      "         150       0.18      0.63      0.27     11851\n",
      "         151       0.00      0.00      0.00        13\n",
      "         152       0.31      0.26      0.28       305\n",
      "         153       0.20      0.02      0.03      4730\n",
      "         154       0.33      0.04      0.07      5151\n",
      "         155       0.00      0.00      0.00        34\n",
      "         156       0.00      0.00      0.00        55\n",
      "         157       0.47      0.39      0.43      2714\n",
      "         158       0.00      0.00      0.00        14\n",
      "         159       0.08      0.03      0.05       213\n",
      "         160       0.00      0.00      0.00       156\n",
      "         161       0.02      0.01      0.01      1081\n",
      "         162       0.05      0.01      0.01       605\n",
      "         163       0.00      0.00      0.00        15\n",
      "         164       0.01      0.00      0.00       691\n",
      "         165       0.00      0.00      0.00         7\n",
      "         166       0.02      0.01      0.01       324\n",
      "         167       0.00      0.00      0.00        85\n",
      "         168       0.00      0.00      0.00        71\n",
      "         169       0.04      0.02      0.03       257\n",
      "         170       0.01      0.01      0.01        77\n",
      "         171       0.01      0.01      0.01        95\n",
      "         172       0.00      0.00      0.00       135\n",
      "         173       0.00      0.00      0.00       144\n",
      "         174       0.26      0.15      0.19      1707\n",
      "         175       0.00      0.00      0.00         2\n",
      "         176       0.00      0.00      0.00        17\n",
      "         177       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.15     78886\n",
      "   macro avg       0.04      0.04      0.03     78886\n",
      "weighted avg       0.16      0.15      0.11     78886\n",
      "\n",
      "Precision (weighted): 0.16263267856210012\n",
      "Recall (weighted): 0.14656593058337347\n",
      "F1 Score (weighted): 0.10697345527719124\n",
      "Root Mean Squared Error (RMSE): 72.66997739216258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kzaremski/Library/Mobile Documents/com~apple~CloudDocs/School/CS 3120 001 - Machine Learning/CS 3120 Project/denver-crime-heatmap/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kzaremski/Library/Mobile Documents/com~apple~CloudDocs/School/CS 3120 001 - Machine Learning/CS 3120 Project/denver-crime-heatmap/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kzaremski/Library/Mobile Documents/com~apple~CloudDocs/School/CS 3120 001 - Machine Learning/CS 3120 Project/denver-crime-heatmap/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "# Get a copy of the initial data frame\n",
    "df = original_df.copy()\n",
    "\n",
    "# Encode the target variable\n",
    "encoder = LabelEncoder()\n",
    "df['OFFENSE_TYPE_ID'] = encoder.fit_transform(df['OFFENSE_TYPE_ID'])\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('OFFENSE_TYPE_ID', axis=1)\n",
    "y = df['OFFENSE_TYPE_ID']\n",
    "\n",
    "# Remove rare classes with only 1 instance\n",
    "class_counts = y.value_counts()\n",
    "rare_classes = class_counts[class_counts < 2].index\n",
    "\n",
    "# Filter out the rare classes from the dataset\n",
    "filtered_df = df[~df['OFFENSE_TYPE_ID'].isin(rare_classes)]\n",
    "\n",
    "# Update X and y after removing rare classes\n",
    "X = filtered_df.drop('OFFENSE_TYPE_ID', axis=1)\n",
    "y = filtered_df['OFFENSE_TYPE_ID']\n",
    "\n",
    "# Train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Ensure no class is left with fewer than 2 members in either train or test\n",
    "train_class_counts = y_train.value_counts()\n",
    "test_class_counts = y_test.value_counts()\n",
    "\n",
    "# Print out the counts for train and test to debug\n",
    "print(\"Train Class Distribution:\\n\", train_class_counts)\n",
    "print(\"Test Class Distribution:\\n\", test_class_counts)\n",
    "\n",
    "# Check if any class is left with 1 sample in the train set\n",
    "if any(train_class_counts < 2):\n",
    "    print(\"Some classes in the train set have less than 2 samples. Consider further reducing or combining classes.\")\n",
    "else:\n",
    "    # Initialize and train the model\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Classification report (includes precision, recall, F1-score)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Precision, Recall, and F1 Score\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"Precision (weighted): {precision}\")\n",
    "    print(f\"Recall (weighted): {recall}\")\n",
    "    print(f\"F1 Score (weighted): {f1}\")\n",
    "\n",
    "    # RMSE (Root Mean Squared Error)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "    # To decode the predictions back to the original labels\n",
    "    decoded_predictions = encoder.inverse_transform(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime Likelihood Model (Gradient Boosting Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.11939084745654627\n",
      "Root Mean Squared Error (RMSE): 0.3455298069002821\n",
      "Mean Absolute Error (MAE): 0.159967785987433\n",
      "R2 Score: 0.05250843069410738\n"
     ]
    }
   ],
   "source": [
    "# Get a copy of the initial data frame\n",
    "df = original_df.copy()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Group by grid cell, day of year, and hour to count crimes\n",
    "crime_counts = df.groupby(['X_BLOCK', 'Y_BLOCK', 'DAY_OF_YEAR', 'HOUR']).size().reset_index(name='CRIME_COUNT')\n",
    "\n",
    "# Features and target\n",
    "X = crime_counts[['X_BLOCK', 'Y_BLOCK', 'DAY_OF_YEAR', 'HOUR']]\n",
    "y = crime_counts['CRIME_COUNT']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = HistGradientBoostingRegressor()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Conclusions & Evaluations of These Models\n",
    "The regression model doesn't perform very well, with a low R² score of 0.052, meaning it doesn’t explain much of the variation in crime counts. The Mean Squared Error and Root Mean Squared Error suggest there's still a lot of room for improvement, with predictions off by about 0.35 crimes on average. The classification model also struggles, with low precision, recall, and F1 scores, pointing to issues with class imbalance and poor overall prediction accuracy. Although the accuracy is a bit higher, it doesn't capture the full picture of how well the model is performing across all classes. Both models need better feature engineering and more work on handling imbalances to improve their results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Alternatives: Neural Networks\n",
    "Given the poor performance of my initial models, I am now exploring neural networks, including deep neural networks. With a limited number of features available and the infeasibility of incorporating OSM data, I believe that more complex models like these could help improve predictions. Neural networks are well-suited to handle intricate patterns in data and may offer better performance compared to the models I've tried so far. This approach seems like a promising next step to tackle the issues with accuracy and model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.9918\n",
      "Epoch [11/100], Loss: 0.1463\n",
      "Epoch [21/100], Loss: 0.1354\n",
      "Epoch [31/100], Loss: 0.1272\n",
      "Epoch [41/100], Loss: 0.1263\n",
      "Epoch [51/100], Loss: 0.1261\n",
      "Epoch [61/100], Loss: 0.1255\n",
      "Epoch [71/100], Loss: 0.1253\n",
      "Epoch [81/100], Loss: 0.1252\n",
      "Epoch [91/100], Loss: 0.1252\n",
      "Mean Squared Error (MSE): 0.1262\n",
      "Root Mean Squared Error (RMSE): 0.3552\n",
      "Mean Absolute Error (MAE): 0.1631\n",
      "R2 Score: -0.0014\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Get a copy of the initial data frame\n",
    "df = original_df.copy()\n",
    "\n",
    "# Group by grid cell, day of year, and hour to count crimes\n",
    "crime_counts = df.groupby(['X_BLOCK', 'Y_BLOCK', 'DAY_OF_YEAR', 'HOUR']).size().reset_index(name='CRIME_COUNT')\n",
    "\n",
    "X = crime_counts[['X_BLOCK', 'Y_BLOCK', 'DAY_OF_YEAR', 'HOUR']].values\n",
    "y = crime_counts['CRIME_COUNT'].values\n",
    "\n",
    "# Split data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "class CrimeLikelihoodNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrimeLikelihoodNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.bn5 = nn.BatchNorm1d(16)\n",
    "        self.fc6 = nn.Linear(16, 1) \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = torch.relu(self.bn4(self.fc4(x)))\n",
    "        x = torch.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "model = CrimeLikelihoodNeuralNetwork(input_dim)\n",
    "\n",
    "# Check if MPS (MacBook Metal GPU support) is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "\n",
    "# Moving the model and data to the new device if it is enabled\n",
    "model.to(device)\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "number_epochs = 100\n",
    "for epoch in range(number_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{number_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "    \n",
    "    # Move the tensor to CPU before converting to NumPy array\n",
    "    y_pred = y_pred_tensor.cpu().numpy()\n",
    "\n",
    "    # Calculate RMSE, MAE, and R2\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print key metrics\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
